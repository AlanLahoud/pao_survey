{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "#### This tutorial is connected to the paper \"Predict-and-Optimize: A survey on problem variations and approaches.\" Here, we aim to show that finding data-driven decisions (from an incomplete but predictable Optimization Problem) using Gradient Descent approaches with Operational loss functions (combined approaches) usually is better than separating your predictions from the Optimization Problem (decoupled approaches).\n",
    "\n",
    "##### Decoupled approaches: First uses Machine Learning techniques to find predictions. And then uses those predictions to formulate and solve an Optimization Problem.\n",
    "\n",
    "##### Combined approaches: Uses Machine Learning and Mathematical Programming techniques in each step of the learning to solve the incomplete Optimization Problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Table of content}}$<br>\n",
    "\n",
    "$\\mathbf{\\text{0. Data}}$<br>\n",
    "\n",
    "$\\mathbf{\\text{1. Vanilla Newsvendor Problem}}$<br>\n",
    "\n",
    "1.1 Decoupled approach\n",
    "\n",
    "1.2 Combined approach\n",
    "\n",
    "1.3 Regret evaluation\n",
    "\n",
    "$\\mathbf{\\text{2. Adding a Budget constraint}}$<br>\n",
    "\n",
    "2.1 Decoupled approach\n",
    "   \n",
    "2.2 Combined state-of-the-art approach (KKT differentiation)\n",
    "\n",
    "2.3 Regret evaluation\n",
    "\n",
    "$\\mathbf{\\text{3. Adding Integrality constraints (Discrete OP)}}$<br>\n",
    "\n",
    "3.1 Decoupled approach\n",
    "\n",
    "3.2 Combined state-of-the-art approach (Gomory cuts + KKT differentiation)\n",
    "\n",
    "3.3 Regret evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import feat_eng as fe\n",
    "import data_selector_items as dsi\n",
    "import params_newsvendor as prm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from mip import Model, xsum, minimize, INTEGER, CONTINUOUS, CutType, OptimizationStatus\n",
    "\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = False\n",
    "dev = torch.device('cpu')  \n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "    dev = torch.device('cuda')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seeds to allow replication\n",
    "# Changing the seed might require hyperparameter tuning again\n",
    "# Because it changes the deterministic parameters\n",
    "seed_number = 0\n",
    "np.random.seed(seed_number)\n",
    "torch.manual_seed(seed_number)\n",
    "random.seed(seed_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of data files\n",
    "path_data = './data/'\n",
    "\n",
    "# Read historical data\n",
    "sales = pd.read_csv(path_data + 'sales_train_evaluation.csv')\n",
    "\n",
    "# Spliting the data in days\n",
    "start_tr_day = 500\n",
    "start_val_day = 1742\n",
    "start_test_day = 1842\n",
    "end_day = 1941\n",
    "\n",
    "# N of items to use (Lower will be much faster)\n",
    "# In the paper, this is d_z\n",
    "n_items = 100\n",
    "\n",
    "# All useful items\n",
    "sku_ids = dsi.select_items(sales, start_tr_day)\n",
    "\n",
    "# Sample only n_items to use\n",
    "random.seed(seed_number)\n",
    "sku_ids = random.sample(sku_ids, n_items)\n",
    "sku_ids = list(set(sku_ids))\n",
    "n_items = len(sku_ids)\n",
    "\n",
    "\n",
    "# Build training and test from historical data\n",
    "data_train, data_val, data_test, feat, n_items = fe.build_data(\n",
    "    path_data, sales, sku_ids, \n",
    "    start_tr_day, start_val_day, start_test_day, end_day)\n",
    "\n",
    "data_train.fillna(0, inplace=True)\n",
    "data_val.fillna(0, inplace=True)\n",
    "data_test.fillna(0, inplace=True)\n",
    "\n",
    "dx = len(feat)\n",
    "\n",
    "# Number of batch_size samples in the SGDs\n",
    "batch_size = 32 # Number of days for combined approaches\n",
    "\n",
    "# Here we change a bit the test data in order to increase the integrality gap \n",
    "# of the optimization problem (didactic purpose). Otherwise the Continuous and \n",
    "# Discrete version would have approximately the same results.\n",
    "data_train['qty'] = data_train['qty']*np.random.normal(1, 0.07)\n",
    "data_val['qty'] = data_val['qty']*np.random.normal(1, 0.07)\n",
    "data_test['qty'] = data_test['qty']*np.random.normal(1, 0.07)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_train[feat])\n",
    "\n",
    "data_train.loc[:, feat] = scaler.transform(data_train[feat])\n",
    "data_val.loc[:, feat] = scaler.transform(data_val[feat])\n",
    "data_test.loc[:, feat] = scaler.transform(data_test[feat])\n",
    "        \n",
    "X_train = torch.tensor(np.array(data_train[feat]).astype('double'), requires_grad= True, device=dev)\n",
    "y_train = torch.tensor(np.array(data_train['qty']).astype('double'), requires_grad= True, device=dev)\n",
    "\n",
    "X_val = torch.tensor(np.array(data_val[feat]).astype('double'), requires_grad= True, device=dev)\n",
    "y_val = torch.tensor(np.array(data_val['qty']).astype('double'), requires_grad= True, device=dev)\n",
    "\n",
    "X_test = torch.tensor(np.array(data_test[feat]).astype('double'), requires_grad= True, device=dev)\n",
    "y_test = torch.tensor(np.array(data_test['qty']).astype('double'), requires_grad= True, device=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(data):\n",
    "    batches_idx = []\n",
    "    n_batches = int(np.floor(data['d'].nunique() / batch_size))\n",
    "    for i in range(0, n_batches):\n",
    "        days = data['d'].unique()\n",
    "        idx = data[data['d'].isin(\n",
    "            np.random.choice(days, batch_size, replace=False))].index.tolist()\n",
    "        if len(idx) == n_items*batch_size:\n",
    "            batches_idx.append(idx)\n",
    "    return batches_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vanilla Newsvendor Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Notation}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "&f_{day}: \\text{Objective (Cost) Function for the } I^{th} \\text{ day of sales}\\\\\n",
    "&z: \\text{Decision Variable (Orders to make for each item)}\\\\\n",
    "&y_I: \\text{Unknown but predictable parameter (Demand for each item for the day I)}\\\\\\\\\n",
    "\\end{align}\n",
    "\n",
    "***\n",
    "$\\mathbf{\\text{Deterministic Parameters}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "&c: \\text{ Linear constant for transport cost }  \\\\\n",
    "&c_s: \\text{Linear constant for shortage cost  }  \\\\\n",
    "&c_w: \\text{Linear constant for excess cost } \\\\\\\\\n",
    "\\end{align}\n",
    "\n",
    "***\n",
    "$\\mathbf{\\text{Objective function}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "f(z,y_I) = c^Tz + c_s^T max(0, y_I-z) + c_w^T max(0, z-y_I)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_t, params_np = prm.get_params(n_items, is_discrete=False, \n",
    "                                     q_factor = 0.01, # Quadratic penalty factor\n",
    "                                     seed_number=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Example of deterministic parameters:', params_t['cs'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_per_item = lambda Z, Y : params_t['c'].to(dev)*Z.to(dev) \\\n",
    "                            + params_t['cs'].to(dev)*torch.max(torch.zeros((n_items)).to(dev),Y.to(dev)-Z.to(dev)) \\\n",
    "                            + params_t['cw'].to(dev)*torch.max(torch.zeros((n_items)).to(dev),Z.to(dev)-Y.to(dev))\n",
    "\n",
    "\n",
    "def reshape_outcomes(y_pred, y):\n",
    "    y_pred = torch.reshape(y_pred, (y_pred.shape[0]//n_items, n_items))\n",
    "    y = torch.reshape(y, (y.shape[0]//n_items, n_items))\n",
    "    return y_pred, y\n",
    "\n",
    "def calc_f_por_item(y_pred, y):\n",
    "    y_pred, y = reshape_outcomes(y_pred, y)\n",
    "    z_star =  argmin_solver(y_pred)\n",
    "    f_per_item = cost_per_item(z_star, y)\n",
    "    return f_per_item\n",
    "\n",
    "def calc_f_per_day(y_pred, y):\n",
    "    f_per_item = calc_f_por_item(y_pred, y)\n",
    "    f = torch.sum(f_per_item, 1)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vanilla Newsvendor Problem - Decoupled Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## Model (h: X->Y) constructor ##################################################\n",
    "#################################################################################\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    # Initialize the layers\n",
    "    def __init__(self, n_feat):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(n_feat, 6).double()\n",
    "        self.bn = nn.BatchNorm1d(6).double()\n",
    "        self.linear2 = nn.Linear(6, 1).double()\n",
    "    \n",
    "    # Perform the computation\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.act1(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# Decoupled approach (mse loss)\n",
    "h = ANN(n_feat=dx).to(dev)\n",
    "opt_h = torch.optim.Adam(h.parameters(), lr=0.001)\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "def train_one_epoch(X_train, y_train, loss_function, optimizer, model):\n",
    "    \n",
    "    batches_idx = generate_batches(data_train)\n",
    "    \n",
    "    for b in batches_idx:\n",
    "        x_tr = X_train[b]\n",
    "        y_tr = y_train[b]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x_tr)\n",
    "\n",
    "        train_loss = loss_function(preds.reshape(-1), y_tr.reshape(-1))\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "def validate_one_epoch(X, y, model, data):\n",
    "    \n",
    "    batches_idx = generate_batches(data)\n",
    "    f_sum = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for b in batches_idx:\n",
    "            x_ = X[b]\n",
    "            y_ = y[b]\n",
    "    \n",
    "            f_ = cost_fn(model(x_).reshape(-1), y_.reshape(-1))  \n",
    "            f_sum = f_sum + f_\n",
    "\n",
    "    return f_sum/len(batches_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "rmse_costs_sep = []\n",
    "\n",
    "for i in range(0, n_epochs):\n",
    "    train_one_epoch(X_train, y_train, mse_loss, opt_h, h)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        train_rmse_sep = mse_loss(h(X_train).reshape(-1), y_train.reshape(-1))  \n",
    "        val_rmse_sep = mse_loss(h(X_val).reshape(-1), y_val.reshape(-1))\n",
    "\n",
    "        rmse_costs_sep.append(val_rmse_sep.data.item())\n",
    "\n",
    "        print(\n",
    "              'DECOUPLED: Train: ', \n",
    "               'mse:', round(train_rmse_sep.data.item(), 2), \n",
    "               '\\tVal: ', \n",
    "               'mse:', round(val_rmse_sep.data.item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Predicting all test outcomes (demand of sales)}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Y} = h(X)\n",
    "\\end{align}\n",
    "\n",
    "##### Note that $Y = [y_I]_1^{N_{days}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = h(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Defining the solver as a function of the prediction (without constraints)}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "z^*(\\hat{y_I}) = argmin_z \\text{ } f(z,\\hat{y_I}) \\\\ subject \\quad to \\quad z\\geq0\n",
    "\\end{align}\n",
    "\n",
    "##### In this case (1.1 and 1.2), we consider an analytical solution to calculate the argmin that allows simple backpropagation via pytorch. This is possible because theproblem is still simple, the only constraint is that z>=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{The analytical solution to find the argmin in this case is trivial:}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "z^*(\\hat{y_I}) = \\hat{y_I} \\\\ \n",
    "\\end{align}\n",
    "\n",
    "This is because we set $c<c_s$ for all items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical solution to find the argmin\n",
    "# This function allows autograd (backpropagation)\n",
    "def argmin_solver(y_pred):\n",
    "    z_star = y_pred\n",
    "    return z_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Compute the final cost function (average through days) based on made decisions}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{N_{days}} \\sum_{I=1}^{N_{days}} f(z^*(\\hat{y_I}),y_I) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_fn(y_pred, y):\n",
    "    f = calc_f_per_day(y_pred, y)\n",
    "    f_total = torch.mean(f)\n",
    "    return f_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cost_11 = cost_fn(y_pred, y_test)\n",
    "print('Final cost on Test Data:', round(pred_cost_11.item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Vanilla Newsvendor Problem - Combined Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined approach (mse loss)\n",
    "hc = ANN(n_feat=dx).to(dev)\n",
    "opt_hc = torch.optim.Adam(hc.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "obj_costs_com = []\n",
    "\n",
    "for i in range(0, n_epochs):\n",
    "    train_one_epoch(X_train, y_train, cost_fn, opt_hc, hc)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        train_cost_com = cost_fn(hc(X_train).reshape(-1), y_train.reshape(-1))  \n",
    "        val_cost_com = cost_fn(hc(X_val).reshape(-1), y_val.reshape(-1))\n",
    "\n",
    "        obj_costs_com.append(val_cost_com.data.item())\n",
    "\n",
    "        print(\n",
    "                  'COMBINED: Train: ', \n",
    "                   'f:', round(train_cost_com.data.item(), 2), \n",
    "                   '\\tVal: ', \n",
    "                   'f:', round(val_cost_com.data.item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Predicting all test outcomes (demand of sales)}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Y} = h(X)\n",
    "\\end{align}\n",
    "\n",
    "Note that $Y = [y_I]_1^{N_{days}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hc(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Compute the final cost function (average through days) based on made decisions}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "PredictedCost = \\frac{1}{N_{days}} \\sum_{I=1}^{N_{days}} f(z^*(\\hat{y_I}),y_I) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cost_12 = cost_fn(y_pred, y_test)\n",
    "print('Final cost on Test Data:', round(pred_cost_12.item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Compute the final cost function (average through days) based on BEST decisions}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "Optimal Cost = \\frac{1}{N_{days}} \\sum_{I=1}^{N_{days}} f(z^*(y_I),y_I) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best1_cost = cost_fn(y_test, y_test)\n",
    "print('Best cost on Test Data:', round(best1_cost.item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Compute the cumulative regrets from 1.1 and 1.2}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "Regret = PredictedCost - OptimalCost\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regret11 = pred_cost_11 - best1_cost\n",
    "regret12 = pred_cost_12 - best1_cost\n",
    "\n",
    "print('Cumulative regret: \\n \\\n",
    "        1.1 -> {regret11} \\n \\\n",
    "        1.2 -> {regret12} \\\n",
    "'.format(\n",
    "    regret11=int(regret11),\n",
    "    regret12=int(regret12)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Compute the normalized regret from 1.1 and 1.2}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "Normalized Regret = \\frac{Regret}{Optimal Cost} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr11 = regret11/best1_cost\n",
    "cr12 = regret12/best1_cost\n",
    "\n",
    "print('Normalized regret: \\n \\\n",
    "        1.1 -> {cr11} \\n \\\n",
    "        1.2 -> {cr12} \\\n",
    "'.format(\n",
    "    cr11=cr11,\n",
    "    cr12=cr12\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Adding a Budget constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This class formulates an Linear Programming (relaxed version of the MILP). We explain how to transform this problem in a Linear Program formulation in Section 6 in our article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolveNewsvendor():\n",
    "    def __init__(self, params_np):\n",
    "        super(SolveNewsvendor, self).__init__()\n",
    "            \n",
    "        n_items = len(params_np['c'])\n",
    "        self.n_items = n_items    \n",
    "            \n",
    "        # Numpy parameters for Gomory cuts\n",
    "        self.cost_vector = np.hstack((params_np['c'], params_np['cs'], params_np['cw']))\n",
    "        self.price_ineq_np = np.hstack((params_np['pr'], np.zeros(2*n_items)) )\n",
    "        self.size_ineq_np = np.hstack((params_np['si'], np.zeros(2*n_items)))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def milp_formulation(self, y_I):\n",
    "        \"\"\"\n",
    "        Formulate the MILP to be used in \"solve_milp\"\n",
    "        We can solve the continuous version (LP) just passing relax = True in \"solve_milp\"\n",
    "        The MILP is formulated based on an outcome chunk y_I.\n",
    "        \"\"\"\n",
    "        m = Model(\"milp\")\n",
    "        m.verbose = 0\n",
    "\n",
    "        n_items_range = range(n_items)\n",
    "        n_variables = range(3*n_items)\n",
    "        \n",
    "        z = ([m.add_var(var_type=INTEGER) for i in range(0, n_items)] \n",
    "             + [m.add_var(var_type=CONTINUOUS) for i in range(n_items, 3*n_items)])\n",
    "\n",
    "        # linear objective function\n",
    "        m.objective = minimize(xsum(self.cost_vector[i] * z[i] for i in n_variables))\n",
    "\n",
    "        # all variables greater than zero\n",
    "        for i in n_variables:\n",
    "            m += -z[i] <= 0\n",
    "        # constraints on shortage variables\n",
    "        for i in range(0, n_items):\n",
    "            m += -z[i] - z[i + n_items] <= -y_I[i]\n",
    "        # constraints on excess variables\n",
    "        for i in range(0, n_items):\n",
    "            m += z[i] - z[i + 2*n_items] <= y_I[i]\n",
    "        # constraints on budget\n",
    "        m += xsum(self.price_ineq_np[i] * z[i] for i in n_variables) <= params_np['B']\n",
    "        \n",
    "        return m\n",
    "    \n",
    "    \n",
    "    def solve_milp(self, y, relax):\n",
    "        \"\"\"\n",
    "        The function solves the problem for each y_I in a batch\n",
    "        and return the argmin result for the whole batch\n",
    "        \"\"\"\n",
    "        y = y.cpu().detach().numpy()\n",
    "        n_batches, n_items = y.shape\n",
    "        \n",
    "        argmin = []\n",
    "        \n",
    "        for j in range(0, n_batches):\n",
    "            y_I = y[j]\n",
    "            m = self.milp_formulation(y_I)\n",
    "            status = m.optimize(relax = relax)\n",
    "        \n",
    "            argmin_sample = []\n",
    "            for v in m.vars:\n",
    "                argmin_sample.append(v.x)\n",
    "            argmin.append(argmin_sample[:n_items])\n",
    "             \n",
    "        return torch.tensor(argmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Budget (sum of prices) upperbound (constraint)\n",
    "params_np['B'] = (params_np['pr'].sum()*y_test.median().item()*0.6).astype('float32')\n",
    "params_t['B'] = torch.tensor([params_np['B']])\n",
    "\n",
    "print('Example of price constant and budget constraints:', params_np['pr'][:2], round(params_t['B'].item(), 2))\n",
    "\n",
    "# Construct the solver\n",
    "newsvendor_solve = SolveNewsvendor(params_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Adding a Budget constraints - Decoupled approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve.solve_milp(y_pred, relax=True) # Solve the continuous version\n",
    "    return z_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Defining the solver as a function of the prediction (with constraints)}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "z^*(\\hat{y_I}) = & argmin_z \\text{ } f(z,\\hat{y_I}) \\\\\n",
    "\\text{Subject to } & \\begin{cases}\n",
    "pr^Tz \\leq B \\\\\n",
    "z \\geq 0\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now the function \"argmin_solver\" is replaced to a linear programming (relaxation of a MILP). This solver does not allow backpropagation in principle, so we will solve first in a decoupled manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The predictions are already done in 1.1 using MSE loss, so we leverage the same predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Compute the final cost function (average through days) based on made decisions}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "PredictedCost = \\frac{1}{N_{days}} \\sum_{I=1}^{N_{days}} f(z^*(\\hat{y_I}),y_I) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = h(X_test)\n",
    "pred_cost_21 = cost_fn(y_pred, y_test)\n",
    "print('Final cost on Test Data:', round(pred_cost_21.item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Adding a Budget constraints - Combined state-of-the-art approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This class formulates a Quadratic Programming using the library QPTH from Amos and Kolter (2017). We formulate the LP now as a QP (addinga penalty quadratic term) proposed by Wilder et al. (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolveNewsvendorWithKKT():\n",
    "    def __init__(self, params_t):\n",
    "        super(SolveNewsvendorWithKKT, self).__init__()\n",
    "            \n",
    "        n_items = len(params_t['c'])\n",
    "        self.n_items = n_items    \n",
    "            \n",
    "        # Torch parameters for KKT         \n",
    "        ident = torch.eye(n_items)\n",
    "        ident3 = torch.eye(3*n_items)\n",
    "        zeros_matrix = torch.zeros((n_items, n_items))\n",
    "        zeros_array = torch.zeros(n_items)\n",
    "        ones_array = torch.ones(n_items)\n",
    "             \n",
    "        self.Q = torch.diag(torch.hstack((params_t['q'], params_t['qs'], params_t['qw']))).to(dev)\n",
    "        self.lin = torch.hstack((params_t['c'], params_t['cs'], params_t['cw'])).to(dev)\n",
    "             \n",
    "        shortage_ineq = torch.hstack((-ident, -ident, zeros_matrix))\n",
    "        excess_ineq = torch.hstack((ident, zeros_matrix, -ident))\n",
    "        price_ineq = torch.hstack((params_t['pr'], zeros_array, zeros_array))\n",
    "        positive_ineq = -ident3\n",
    "        \n",
    "        self.ineqs = torch.vstack((shortage_ineq, excess_ineq, price_ineq, positive_ineq)).to(dev)\n",
    "\n",
    "        self.uncert_bound = torch.hstack((-ones_array, ones_array)).to(dev)\n",
    "        self.determ_bound = torch.tensor([params_t['B']]) \n",
    "        self.determ_bound = torch.hstack((self.determ_bound, \n",
    "                                          torch.zeros(n_items), \n",
    "                                          torch.zeros(n_items), \n",
    "                                          torch.zeros(n_items))).to(dev)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, y):\n",
    "        \"\"\"\n",
    "        Applies the qpth solver for all batches and allows backpropagation.\n",
    "        Formulation based on Priya L. Donti, Brandon Amos, J. Zico Kolter (2017).\n",
    "        Note: The quadratic terms (Q) are used as auxiliar terms only to allow the backpropagation through the \n",
    "        qpth library from Amos and Kolter. \n",
    "        We will set them as a small percentage of the linear terms (Wilder, Ewing, Dilkina, Tambe, 2019)\n",
    "        \"\"\"\n",
    "        \n",
    "        n_batches, n_items = y.size()\n",
    "        \n",
    "        assert self.n_items == n_items \n",
    "\n",
    "        Q = self.Q\n",
    "        Q = Q.expand(n_batches, Q.size(0), Q.size(1))\n",
    "\n",
    "        ineqs = torch.unsqueeze(self.ineqs, dim=0)\n",
    "        ineqs = ineqs.expand(n_batches, ineqs.shape[1], ineqs.shape[2])       \n",
    "\n",
    "        uncert_bound = (self.uncert_bound*torch.hstack((y, y)))\n",
    "        determ_bound = self.determ_bound.unsqueeze(dim=0).expand(\n",
    "            n_batches, self.determ_bound.shape[0])\n",
    "        bound = torch.hstack((uncert_bound, determ_bound))     \n",
    "        \n",
    "        e = torch.DoubleTensor().to(dev)\n",
    "        \n",
    "        argmin = QPFunction(verbose=-1)\\\n",
    "            (Q.double(), self.lin.double(), ineqs.double(), \n",
    "             bound.double(), e, e).double()\n",
    "            \n",
    "        return argmin[:,:n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the solver\n",
    "newsvendor_solve_kkt = SolveNewsvendorWithKKT(params_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined approach (mse loss)\n",
    "hc = ANN(n_feat=dx).to(dev)\n",
    "opt_hc = torch.optim.Adam(hc.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Defining the solver as a function of the prediction (with constraints)}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "z^*(\\hat{y_I}) = & argmin_z \\text{ } f(z,\\hat{y_I}) \\\\\n",
    "\\text{Subject to } & \\begin{cases}\n",
    "pr^Tz \\leq B \\\\\n",
    "z \\geq 0\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now the function \"argmin_solver\" is replaced to a quadratic programming that allows backpropagation, and we use the method proposed by Priya Donti et al. (2017) to solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_kkt.forward(y_pred)\n",
    "    return z_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "obj_costs_com = []\n",
    "\n",
    "for i in range(0, n_epochs):\n",
    "    train_one_epoch(X_train, y_train, cost_fn, opt_hc, hc)\n",
    "    \n",
    "    f_train = validate_one_epoch(X_train, y_train, hc, data_train)\n",
    "    f_val = validate_one_epoch(X_val, y_val, hc, data_val)\n",
    "\n",
    "    print(\n",
    "              'COMBINED: Train: ', \n",
    "               'f:', round(f_train.data.item(), 2), \n",
    "               '\\tVal: ', \n",
    "               'f:', round(f_val.data.item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Predicting all test outcomes (demand of sales)}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Y} = hc(X)\n",
    "\\end{align}\n",
    "\n",
    "##### Note that $Y = [y_I]_1^{N_{days}}$\n",
    "\n",
    "##### Also, note that now $hc$ was learned based on the cost function through the KKT differentiation, not on the MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hc(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve.solve_milp(y_pred, relax=True)\n",
    "    return z_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Compute the final cost function (average through days) based on made decisions}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "PredictedCost = \\frac{1}{N_{days}} \\sum_{I=1}^{N_{days}} f(z^*(\\hat{y_I}),y_I) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cost_22 = cost_fn(y_pred, y_test)\n",
    "print('Final cost on Test Data:', round(pred_cost_22.item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Compute the final cost function (average through days) based on BEST decisions}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "OptimalCost = \\frac{1}{N_{days}} \\sum_{I=1}^{N_{days}} f(z^*(y_I),y_I) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best2_cost = cost_fn(y_test, y_test)\n",
    "print('Best cost on Test Data:', round(best2_cost.item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Compute the cumulative regrets from 2.1 and 2.2}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "Regret = PredictedCost - OptimalCost\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regret21 = pred_cost_21 - best2_cost\n",
    "regret22 = pred_cost_22 - best2_cost\n",
    "\n",
    "print('Cumulative regret: \\n \\\n",
    "        2.1 -> {regret21} \\n \\\n",
    "        2.2 -> {regret22} \\\n",
    "'.format(\n",
    "    regret21=int(regret21),\n",
    "    regret22=int(regret22)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Compute the normalized regret from 2.1 and 2.2}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "Normalized Regret = \\frac{Regret}{Optimal Cost} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr21 = regret21/best2_cost\n",
    "cr22 = regret22/best2_cost\n",
    "\n",
    "print('Normalized regret: \\n \\\n",
    "        2.1 -> {cr21} \\n \\\n",
    "        2.2 -> {cr22} \\\n",
    "'.format(\n",
    "    cr21=cr21,\n",
    "    cr22=cr22\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Adding integrality constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Adding integrality constraints - Decoupled Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By adding integrality constraints, we aim (at the final moment) to solve the same problem with the parameter relax = False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Defining the solver as a function of the prediction (with constraints)}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "z^*(\\hat{y_I}) = & argmin_z \\text{ } f(z,\\hat{y_I}) \\\\\n",
    "\\text{Subject to } & \\begin{cases}\n",
    "pr^Tz \\leq B \\\\\n",
    "z \\geq 0 \\\\\n",
    "z \\in \\mathbb{Z}\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve.solve_milp(y_pred, relax=False)\n",
    "    return z_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The predictions are already done in 1.1 using MSE loss, so we leverage the same predictions\n",
    "\n",
    "***\n",
    "$\\mathbf{\\text{Compute the final cost function (average through days) based on made decisions}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "PredictedCost = \\frac{1}{N_{days}} \\sum_{I=1}^{N_{days}} f(z^*(\\hat{y_I}),y_I) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = h(X_test)\n",
    "pred_cost_31 = cost_fn(y_pred, y_test)\n",
    "print('Final cost on Test Data:', round(pred_cost_31.item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Adding integrality constraints - Combined Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each step of the gradient descent we will solve the OP with relax = True after applying the Gomory cuts Aaron Ferber et al. (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of maximum cuts for each iteration of the learning\n",
    "N_try_cuts = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolveNewsvendorWithGomoryAndKKT():\n",
    "    def __init__(self, params_t, params_np):\n",
    "        super(SolveNewsvendorWithGomoryAndKKT, self).__init__()\n",
    "            \n",
    "        n_items = len(params_np['c'])\n",
    "        self.n_items = n_items    \n",
    "            \n",
    "        # Numpy parameters for Gomory cuts\n",
    "        self.cost_vector = np.hstack((params_np['c'], params_np['cs'], params_np['cw']))\n",
    "        self.price_ineq_np = np.hstack((params_np['pr'], np.zeros(2*n_items)) )\n",
    "        \n",
    "        # Torch parameters for KKT         \n",
    "        ident = torch.eye(n_items)\n",
    "        ident3 = torch.eye(3*n_items)\n",
    "        zeros_matrix = torch.zeros((n_items, n_items))\n",
    "        zeros_array = torch.zeros(n_items)\n",
    "        ones_array = torch.ones(n_items)\n",
    "             \n",
    "        self.Q = torch.diag(torch.hstack((params_t['q'], params_t['qs'], params_t['qw']))).to(dev)\n",
    "        self.lin = torch.hstack((params_t['c'], params_t['cs'], params_t['cw'])).to(dev)\n",
    "             \n",
    "        shortage_ineq = torch.hstack((-ident, -ident, zeros_matrix))\n",
    "        excess_ineq = torch.hstack((ident, zeros_matrix, -ident))\n",
    "        price_ineq = torch.hstack((params_t['pr'], zeros_array, zeros_array))\n",
    "        positive_ineq = -ident3\n",
    "        \n",
    "        self.ineqs = torch.vstack((shortage_ineq, excess_ineq, price_ineq, positive_ineq)).to(dev)\n",
    "\n",
    "        self.uncert_bound = torch.hstack((-ones_array, ones_array)).to(dev)\n",
    "        self.determ_bound = torch.tensor([params_t['B']]) \n",
    "        self.determ_bound = torch.hstack((self.determ_bound, \n",
    "                                          torch.zeros(n_items), \n",
    "                                          torch.zeros(n_items), \n",
    "                                          torch.zeros(n_items))).to(dev)\n",
    "\n",
    "        \n",
    "    def milp_formulation(self, y_I):\n",
    "        \"\"\"\n",
    "        Formulate the MILP to be used in \"solve_milp\" or in \"forward_gomory_cut\"\n",
    "        The MILP is formulated based on an outcome chunk y_I.\n",
    "        \"\"\"\n",
    "        m = Model(\"milp\")\n",
    "        m.verbose = 0\n",
    "\n",
    "        n_items_range = range(n_items)\n",
    "        n_variables = range(3*n_items)\n",
    "        \n",
    "        z = ([m.add_var(var_type=INTEGER) for i in range(0, n_items)] \n",
    "             + [m.add_var(var_type=CONTINUOUS) for i in range(n_items, 3*n_items)])\n",
    "\n",
    "        # linear objective function\n",
    "        m.objective = minimize(xsum(self.cost_vector[i] * z[i] for i in n_variables))\n",
    "\n",
    "        # all variables greater than zero\n",
    "        for i in n_variables:\n",
    "            m += -z[i] <= 0\n",
    "        # constraints on shortage variables\n",
    "        for i in range(0, n_items):\n",
    "            m += -z[i] - z[i + n_items] <= -y_I[i]\n",
    "        # constraints on excess variables\n",
    "        for i in range(0, n_items):\n",
    "            m += z[i] - z[i + 2*n_items] <= y_I[i]\n",
    "        # constraints on budget\n",
    "        m += xsum(self.price_ineq_np[i] * z[i] for i in n_variables) <= params_np['B']\n",
    "        \n",
    "        return m\n",
    "    \n",
    "    \n",
    "    def one_sample_cuts(self, y_I):\n",
    "        \"\"\"\n",
    "        This function first formulates the MILP and then uses the MILP formulation to \n",
    "        compute and return some Gomory cuts (just for 1 instance of I (day))\n",
    "        \"\"\"\n",
    "        m = self.milp_formulation(y_I)  \n",
    "        n_cuts = 0\n",
    "\n",
    "        for i in range(N_try_cuts):\n",
    "            status = m.optimize(relax = True)\n",
    "            cp = m.generate_cuts([CutType.GOMORY])\n",
    "\n",
    "            if cp.cuts:\n",
    "                if n_cuts + len(cp.cuts) <= N_try_cuts:\n",
    "                    n_cuts = n_cuts + len(cp.cuts)\n",
    "                    m += cp\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "        matrix_ineq_cut = np.array([])\n",
    "        array_ineq_bound = np.array([])\n",
    "        for i in range(-n_cuts, 0):\n",
    "            cut = m.constrs[-n_cuts:][i].expr.expr\n",
    "            varslist = list(m.vars)\n",
    "            dict_vars_init = dict(zip(varslist, len(varslist)*[0]))\n",
    "            ineq_cut = {**dict_vars_init, **cut}\n",
    "            values_ineq_cut = np.array(list(ineq_cut.values()))\n",
    "            array_ineq_bound = np.append(array_ineq_bound, -m.constrs[i].expr.const)\n",
    "            if i == -n_cuts:\n",
    "                matrix_ineq_cut = values_ineq_cut\n",
    "            else:\n",
    "                matrix_ineq_cut = np.vstack((matrix_ineq_cut, values_ineq_cut))\n",
    "\n",
    "        # Adding zeros to standardize ineq size\n",
    "        if n_cuts == 0:\n",
    "            matrix_ineq_cut = np.zeros((N_try_cuts-n_cuts, 3*n_items))\n",
    "            array_ineq_bound = np.ones(N_try_cuts-n_cuts)\n",
    "\n",
    "        elif n_cuts < N_try_cuts:\n",
    "            zrsm = np.zeros((N_try_cuts-n_cuts, 3*n_items))\n",
    "            zrsv = np.ones(N_try_cuts-n_cuts)\n",
    "            matrix_ineq_cut = np.vstack((matrix_ineq_cut, zrsm))\n",
    "            array_ineq_bound = np.hstack((array_ineq_bound, zrsv))\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        array_ineq_bound = array_ineq_bound.reshape(-1)\n",
    "\n",
    "        matrix_ineq_cut = np.expand_dims(matrix_ineq_cut, 0)\n",
    "        array_ineq_bound = np.expand_dims(array_ineq_bound, 0)\n",
    "\n",
    "        cut = np.dstack((matrix_ineq_cut, array_ineq_bound))\n",
    "\n",
    "        return cut\n",
    "\n",
    "    \n",
    "    def forward_gomory_cut(self, y):\n",
    "        \"\"\"\n",
    "        This function applies N_try_cuts in the relaxed version of the MILP\n",
    "        formulation from \"milp_formulation\". The function returns the matrix \"mic\"\n",
    "        and the vector \"aic\" for each \"y_I\". Those outputs represent the cuts in a\n",
    "        form of inequalities mic*z<=aic. \n",
    "        Idea of Gomory cuts from Ferber, Wilder, Dilkina and Tambe (2019).\n",
    "        \"\"\"\n",
    "        \n",
    "        y = y.cpu().detach().numpy()\n",
    "        n_batches, n_items = y.shape\n",
    "\n",
    "        mic = np.array([])\n",
    "        aib = np.array([])\n",
    "        \n",
    "        if is_cuda:\n",
    "            cuts = []\n",
    "            for y_I in y:\n",
    "                cuts.append(self.one_sample_cuts(y_I))\n",
    "        else:\n",
    "            # The code below allows parallelization while cutting. Todo: fix for CUDA.\n",
    "            cuts = Parallel(n_jobs=-1, backend='multiprocessing')(delayed(self.one_sample_cuts)(y_I) for y_I in y) \n",
    "        \n",
    "        cuts = np.vstack(cuts)\n",
    "        mic = cuts[:,:,:-1]\n",
    "        aic = cuts[:,:,-1]\n",
    "\n",
    "        return torch.tensor(mic), torch.tensor(aic)\n",
    "        \n",
    "       \n",
    "    def forward(self, y):\n",
    "        \"\"\"\n",
    "        Applies the qpth solver for all batches and allows backpropagation.\n",
    "        Formulation based on Priya L. Donti, Brandon Amos, J. Zico Kolter (2017).\n",
    "        Note: The quadratic terms (Q) are used as auxiliar terms only to allow the backpropagation through the \n",
    "        qpth library from Amos and Kolter. \n",
    "        We will set them as a small percentage of the linear terms (Wilder, Ewing, Dilkina, Tambe, 2019)\n",
    "        \"\"\"\n",
    "        \n",
    "        n_batches, n_items = y.size()\n",
    "        \n",
    "        assert self.n_items == n_items \n",
    "\n",
    "        Q = self.Q\n",
    "        Q = Q.expand(n_batches, Q.size(0), Q.size(1))\n",
    "\n",
    "        ineqs = torch.unsqueeze(self.ineqs, dim=0)\n",
    "        ineqs = ineqs.expand(n_batches, ineqs.shape[1], ineqs.shape[2])       \n",
    "\n",
    "        uncert_bound = (self.uncert_bound*torch.hstack((y, y)))\n",
    "        determ_bound = self.determ_bound.unsqueeze(dim=0).expand(\n",
    "            n_batches, self.determ_bound.shape[0])\n",
    "        bound = torch.hstack((uncert_bound, determ_bound))\n",
    "\n",
    "        # Adding the Gomory cuts as constraints\n",
    "        m, b = self.forward_gomory_cut(y)\n",
    "        ineqs = torch.hstack((ineqs, m.to(dev)))\n",
    "        bound = torch.hstack((bound, b.to(dev)))    \n",
    "        \n",
    "        e = torch.DoubleTensor().to(dev)\n",
    "        \n",
    "        argmin = QPFunction(verbose=-1)\\\n",
    "            (Q.double(), self.lin.double(), ineqs.double(), \n",
    "             bound.double(), e, e).double()\n",
    "            \n",
    "        return argmin[:,:n_items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Defining the solver as a function of the prediction (with constraints)}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "z^*(\\hat{y_I}) = & argmin_z \\text{ } f(z,\\hat{y_I}) \\\\\n",
    "\\text{Subject to } & \\begin{cases}\n",
    "pr^Tz \\leq B \\\\\n",
    "z \\geq 0\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note that here we still use the relaxed version of the MILP, but considering the gomory cuts. The difference is that the integrality gap will be lower and hopefully the continuous results will becloser to theoptimal integer decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve_gomory_kkt.forward(y_pred)\n",
    "    return z_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined approach (mse loss)\n",
    "hc = ANN(n_feat=dx).to(dev)\n",
    "opt_hc = torch.optim.Adam(hc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the solver\n",
    "newsvendor_solve_gomory_kkt = SolveNewsvendorWithGomoryAndKKT(params_t, params_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "obj_costs_com = []\n",
    "\n",
    "for i in range(0, n_epochs):\n",
    "    train_one_epoch(X_train, y_train, cost_fn, opt_hc, hc)\n",
    "    \n",
    "    f_train = validate_one_epoch(X_train, y_train, hc, data_train)\n",
    "    f_val = validate_one_epoch(X_val, y_val, hc, data_val)\n",
    "\n",
    "    print(\n",
    "              'COMBINED: Train: ', \n",
    "               'f:', round(f_train.data.item(), 2), \n",
    "               '\\tVal: ', \n",
    "               'f:', round(f_val.data.item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hc(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Defining the solver as a function of the prediction (with constraints)}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "z^*(\\hat{y_I}) = & argmin_z \\text{ } f(z,\\hat{y_I}) \\\\\n",
    "\\text{Subject to } & \\begin{cases}\n",
    "pr^Tz \\leq B \\\\\n",
    "z \\geq 0 \\\\\n",
    "z \\in \\mathbb{Z}\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_solver(y_pred):\n",
    "    z_star = newsvendor_solve.solve_milp(y_pred, relax=False)\n",
    "    return z_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After training hc we need to compute the final decisions. And now the final decisions need to be integers (relax = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Compute the final cost function (average through days) based on made decisions}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "PredictedCost = \\frac{1}{N_{days}} \\sum_{I=1}^{N_{days}} f(z^*(\\hat{y_I}),y_I) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cost_32 = cost_fn(y_pred, y_test)\n",
    "print('Final cost on Test Data:', round(pred_cost_32.item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Compute the final cost function (average through days) based on BEST decisions}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "OptimalCost=\\frac{1}{N_{days}} \\sum_{I=1}^{N_{days}} f(z^*(y_I),y_I) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best3_cost = cost_fn(y_test, y_test)\n",
    "print('Best cost on Test Data:', round(best3_cost.item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Compute the cumulative regrets from 3.1 and 3.2}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "Regret = PredictedCost-OptimalCost\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regret31 = pred_cost_31 - best3_cost\n",
    "regret32 = pred_cost_32 - best3_cost\n",
    "\n",
    "print('Cumulative regret: \\n \\\n",
    "        3.1 -> {regret31} \\n \\\n",
    "        3.2 -> {regret32}  \\\n",
    "'.format(\n",
    "    regret31=int(regret31),\n",
    "    regret32=int(regret32),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Compute the normalized regret from 3.1 and 3.2}}$<br>\n",
    "***\n",
    "\n",
    "\\begin{align}\n",
    "Normalized Regret = \\frac{Regret}{Optimal Cost} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr31 = regret31/best3_cost\n",
    "cr32 = regret32/best3_cost\n",
    "\n",
    "print('Normalized regret: \\n \\\n",
    "        3.1 -> {cr31} \\n \\\n",
    "        3.2 -> {cr32}  \\\n",
    "'.format(\n",
    "    cr31=cr31,\n",
    "    cr32=cr32\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and ideas to practitioners:\n",
    "\n",
    "#### 1. Perks of Decoupled Approaches -> Time saver\n",
    "Decoupled approaches run much faster than combined approaches. Notice that we truncate the data to work only with 50 random items. This is because for state-of-the-art combined approaches we need to use Mathematical Programming techniques iteratively, and it takes too much time mainly if the Optimization Problem is too complex. \n",
    "\n",
    "#### 2. Perks of Combined Approaches -> Money saver\n",
    "Comparing the final results, we normally have (we tested with many different parameters and we consistently had) lower regret using combined approaches. This is expected since the loss function is more aligned with the final goal.\n",
    "\n",
    "#### 3. Mixing Decoupled and Combined approaches\n",
    "We suggest the practitioner to try first updating the weight of the ANN using the MSE loss (1.1 training part). And then start the training using the KKT method (2.2 training part) loading the previous ways. This idea (not with KKT differentiation, but with other methods) was proposed by Mandi et al. (2020)\n",
    "\n",
    "#### 4. Early Stopping\n",
    "One can design the algorithm to stop based on the final operational cost (even though using decoupled approach). Notice that doing that requires to solve an Optimization Problem iteratively as well, but it does not require to solve the OP in every iteration. This technique will probably improve mainly the decoupled approaches results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pao_env",
   "language": "python",
   "name": "pao_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
